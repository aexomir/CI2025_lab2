{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f638300c",
   "metadata": {},
   "source": [
    "Load data from results/ to compare Hill Climber and Evolutionary Strategy performance across all problem instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3668f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3501dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Setup and Helper Utilities ======================\n",
    "\n",
    "# Assuming necessary utility and dataclasses are imported from src/\n",
    "# from src.dataclasses.solution import SolutionResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ce9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_rle(rle_history: Dict[str, List[Union[float, int]]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reconstructs the full fitness history array from Run-Length Encoding (RLE).\n",
    "    RLE format: {'values': [v1, v2, ...], 'counts': [c1, c2, ...]}\n",
    "    \"\"\"\n",
    "    values = rle_history.get('values', [])\n",
    "    counts = rle_history.get('counts', [])\n",
    "    if not values or not counts:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Use numpy.repeat to reconstruct the array\n",
    "    return np.repeat(values, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b5abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results(dir_path: str = '../results/') -> Dict[str, Dict[str, List[Any]]]:\n",
    "    \"\"\"\n",
    "    Loads all SolutionResults (.npy files) from the results directory\n",
    "    and groups them by problem name and solver type.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for filename in os.listdir(dir_path):\n",
    "        if filename.endswith('.npy'):\n",
    "            parts = filename.split('_')\n",
    "            problem_name = '_'.join(parts[1:-2]) # e.g., g_10\n",
    "            solver_type = parts[-2]             # e.g., hc or es\n",
    "            \n",
    "            # Load the single SolutionResults object\n",
    "            try:\n",
    "                # np.load(..., allow_pickle=True).item() is often needed for single object\n",
    "                result_obj = np.load(os.path.join(dir_path, filename), allow_pickle=True).item()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            if problem_name not in all_results:\n",
    "                all_results[problem_name] = {'hc': [], 'es': []}\n",
    "                \n",
    "            all_results[problem_name][solver_type].append(result_obj)\n",
    "            \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4992992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Visualization Function ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b1be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(problem_name: str, problem_data: Dict[str, List[Any]]):\n",
    "    \"\"\"\n",
    "    Generates the 3-panel comparison plot for HC vs ES results.\n",
    "    \"\"\"\n",
    "    hc_results = problem_data.get('hc', [])\n",
    "    es_results = problem_data.get('es', [])\n",
    "    \n",
    "    if not hc_results and not es_results:\n",
    "        print(f\"No results available for {problem_name}\")\n",
    "        return\n",
    "\n",
    "    # Basic Plot Setup\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(18, 15), sharex=False)\n",
    "    fig.suptitle(f\"Solver Performance Comparison: {problem_name}\", fontsize=16)\n",
    "\n",
    "    # --- Panel 1 (Upper): Hill Climber ---\n",
    "    ax = axes[0]\n",
    "    if hc_results:\n",
    "        # Assuming only one HC run or taking the first one\n",
    "        hc_history = decompress_rle(hc_results[0].history)\n",
    "        ax.plot(hc_history, label='HC Fitness', color='black', alpha=0.7)\n",
    "        ax.set_title(\"1. Hill Climber (Current vs Best Solution)\")\n",
    "        ax.set_ylabel(\"Fitness (Cost)\")\n",
    "    else:\n",
    "        ax.set_title(\"1. Hill Climber (No Data)\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    # --- Panel 2 (Middle): ES (Greedy Init) ---\n",
    "    ax = axes[1]\n",
    "    greedy_es = [res for res in es_results if res.params.get('greedy_initial_solutions')]\n",
    "    if greedy_es:\n",
    "        for res in greedy_es:\n",
    "            es_history = decompress_rle(res.history)\n",
    "            label = (f\"Mut={res.params.get('mutation_rate', '?')}, \"\n",
    "                     f\"Pop={res.params.get('population_size', '?')}, \"\n",
    "                     f\"Offs={res.params.get('offspring_size', '?')}\")\n",
    "            ax.plot(es_history, label=label, alpha=0.6)\n",
    "        ax.set_title(\"2. Evolutionary Strategy (Greedy Initialization Overlay)\")\n",
    "        ax.set_ylabel(\"Fitness (Cost)\")\n",
    "        # ax.legend(loc='upper right', ncol=2, fontsize=8) # Omitted for simplicity\n",
    "    else:\n",
    "        ax.set_title(\"2. ES Greedy Init (No Data)\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    # --- Panel 3 (Lower): ES (Random Init) ---\n",
    "    ax = axes[2]\n",
    "    random_es = [res for res in es_results if not res.params.get('greedy_initial_solutions')]\n",
    "    if random_es:\n",
    "        for res in random_es:\n",
    "            es_history = decompress_rle(res.history)\n",
    "            label = (f\"Mut={res.params.get('mutation_rate', '?')}, \"\n",
    "                     f\"Pop={res.params.get('population_size', '?')}, \"\n",
    "                     f\"Offs={res.params.get('offspring_size', '?')}\")\n",
    "            ax.plot(es_history, label=label, alpha=0.6)\n",
    "        ax.set_title(\"3. Evolutionary Strategy (Random Initialization Overlay)\")\n",
    "        ax.set_xlabel(\"Iterations/Generations\")\n",
    "        ax.set_ylabel(\"Fitness (Cost)\")\n",
    "    else:\n",
    "        ax.set_title(\"3. ES Random Init (No Data)\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f09d585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Main Execution Block ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2e1a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all benchmark results...\n",
      "\n",
      "Warning: 'tsp_best_tuned.json' not found. Run benchmarks first.\n",
      "\n",
      "Analysis notebook execution simulated.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 1. Load Data\n",
    "    print(\"Loading all benchmark results...\")\n",
    "    results_data = load_all_results()\n",
    "    \n",
    "    # 2. Generate Plots for each problem\n",
    "    for problem_name, data in results_data.items():\n",
    "        plot_comparison(problem_name, data)\n",
    "        \n",
    "    # 3. Final Summary (loading the final best fitness table)\n",
    "    try:\n",
    "        with open('../results/tsp_best_tuned.json', 'r') as f:\n",
    "            final_best_solutions = json.load(f)\n",
    "            \n",
    "        print(\"\\n================== Final Best Tuned Solutions ==================\")\n",
    "        # Note: In the actual notebook, this would be formatted as a nice table\n",
    "        for name, data in final_best_solutions.items():\n",
    "             print(f\"{name}: Best Fitness = {data['best_fitness']:.4f}\")\n",
    "             \n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nWarning: 'tsp_best_tuned.json' not found. Run benchmarks first.\")\n",
    "        \n",
    "    print(\"\\nAnalysis notebook execution simulated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
